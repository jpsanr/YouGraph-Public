{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from acessos import read, get_conn, persistir_uma_linha, persistir_multiplas_linhas, replace_df, update_banco\n",
    "import pandas as pd\n",
    "from math import*\n",
    "from tqdm.auto import tqdm  # for notebooks\n",
    "from connections.mysql_connector import MySQL_Connector\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard_similarity(x,y):\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    return intersection_cardinality/float(union_cardinality)\n",
    "\n",
    "\n",
    "x = [1,2,3,4,5,6,7,8,9,10]\n",
    "y = [1,1,1,1,1,1,1,1,1,1]\n",
    "jaccard_similarity(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_scores(df_avaliador_1, df_avaliador_2, avaliador_1, avaliador_2, verbose=False):\n",
    "    \n",
    "    #Refatorar esta linha, pois pode dar erro em dataFrames com tamnanhos diferentes!!\n",
    "    lista_sentencas = list(df_avaliador_1['sentenca_id'].unique())\n",
    "\n",
    "    df_scores = pd.DataFrame()\n",
    "\n",
    "    for sentenca_id in tqdm(lista_sentencas): \n",
    "        if verbose == True:\n",
    "            print(\"Sentenca ID {}\".format(sentenca_id))\n",
    "\n",
    "\n",
    "        df_sentencas_av1 = df_avaliador_1.loc[df_avaliador_1['sentenca_id'] == sentenca_id]\n",
    "        df_sentencas_av2 = df_avaliador_2.loc[df_avaliador_2['sentenca_id'] == sentenca_id]\n",
    "\n",
    "\n",
    "        lista_grupos_avaliador_1 = df_sentencas_av1[\"num_grupo\"].unique()\n",
    "        lista_grupos_avaliador_2 = df_sentencas_av2[\"num_grupo\"].unique()\n",
    "\n",
    "#         print(\"  -> Total grupos Av1: {}\".format(len(lista_grupos_avaliador_1)))\n",
    "#         print(\"  -> Total grupos Av2: {}\".format(len(lista_grupos_avaliador_2)))\n",
    "\n",
    "\n",
    "        for i in range(0,len(lista_grupos_avaliador_1)+1):\n",
    "    \n",
    "            lista_tuplas_grupo_av1 = list(df_sentencas_av1.loc[df_sentencas_av1['num_grupo'] == i]['tupla_id'])\n",
    "            \n",
    "            \n",
    "            for j in range(0,len(lista_grupos_avaliador_2)+1):\n",
    "                score = 0  \n",
    "               \n",
    "                lista_tuplas_grupo_av2 = list(df_sentencas_av2.loc[df_sentencas_av2['num_grupo'] == j]['tupla_id'])\n",
    "                \n",
    "                \n",
    "            \n",
    "                #iterando tuplas do grupo metodo 1\n",
    "#                 print(lista_tuplas_grupo_av1)\n",
    "                for t in lista_tuplas_grupo_av1:\n",
    "                    if t in lista_tuplas_grupo_av2:\n",
    "                        score=jaccard_similarity(lista_tuplas_grupo_av1, lista_tuplas_grupo_av2)\n",
    "\n",
    "                    else: \n",
    "                        continue\n",
    "\n",
    "                linha_dataFrame = {\"sentenca_id\": sentenca_id,\n",
    "                                  \"avaliador1\": avaliador_1,\n",
    "                                  \"avaliador2\": avaliador_2,\n",
    "                                  \"grupo_av1\": i,\n",
    "                                  \"grupo_av2\": j,\n",
    "                                  \"score\": score\n",
    "                                  }\n",
    "                df_scores = df_scores.append(linha_dataFrame, ignore_index=True)\n",
    "                if verbose == True:\n",
    "                    print(\"    -> calculando av1 grupo {} com av2 grupo {}\".format(i,j))\n",
    "                    print(\" {}  contra  {}  \".format(lista_tuplas_grupo_av1,lista_tuplas_grupo_av2))\n",
    "                    print(\"        -> Score {}\".format(score))\n",
    "    return df_scores\n",
    "\n",
    "def get_df_tuplas(tag_metodo):\n",
    "    query = '''\n",
    "                SELECT * FROM tupla_grupo tg \n",
    "                    INNER JOIN grupo_resumo gr USING (grupo_id) \n",
    "                    INNER JOIN espaco_amostral_sentenca using(sentenca_id)\n",
    "                WHERE tag_metodo = '{}'\n",
    "            '''.format(tag_metodo)\n",
    "    \n",
    "    return read(conn, query)\n",
    "    \n",
    "def get_tag_metodos():\n",
    "    query =  \"SELECT distinct tag_metodo FROM grupo_resumo;\"\n",
    "    return read(conn, query)\n",
    "\n",
    "def mostrar_boas_vindas():\n",
    "    df_tag_metodos = get_tag_metodos()\n",
    "    lista_tag_metodos = list(df_tag_metodos['tag_metodo'])\n",
    "    print(\"Olá! Favor inserir os parâmetros de execução (avaliador1 e avaliador2):\")\n",
    "    print(\"Selcione os avaliadores a partir dos seguintes tag_metodos:\\n\")\n",
    "    print(lista_tag_metodos )\n",
    "    print(\"\\nInsira o avaliador_1:\")\n",
    "    avaliador_1 = input() \n",
    "    print(\"Insira o avaliador_2:\")\n",
    "    avaliador_2 = input()\n",
    "    return avaliador_1, avaliador_2\n",
    "    \n",
    "\n",
    "def alerta_dataframe_diferente():\n",
    "    print(\"**********\")\n",
    "    print(\"**** Atenção os Métodos possuem tamanhos diferentes!! ****\")\n",
    "    print(\"Deseja continuar? (sim/nao)\")\n",
    "    continuar = input()\n",
    "    if continuar == \"sim\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def buscar_similaridade_grupos(avaliador1, avaliador2, sentenca_id): \n",
    "    query =  '''SELECT * FROM avaliacao_similaridade_grupos \n",
    "                where avaliador1 ='{}' and avaliador2 ='{}' and sentenca_id={} ;'''.format(avaliador1, avaliador2,sentenca_id)\n",
    "    return read(conn, query)\n",
    "\n",
    "def calcular_media(avaliador1, avaliador2, sentenca_id):\n",
    "    df_medias = buscar_similaridade_grupos(avaliador1, avaliador2,sentenca_id)\n",
    "    media = df_medias['score'].mean()\n",
    "    return media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando Similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá! Favor inserir os parâmetros de execução (avaliador1 e avaliador2):\n",
      "Selcione os avaliadores a partir dos seguintes tag_metodos:\n",
      "\n",
      "['word2vec-70-OLD', ' distancia-cosseno-80-OLD', 'bert-cosseno-70-OLD', 'bert-cosseno-80-OLD', 'word2vec-80-OLD', 'distancia-cosseno-70-OLD', 'tsne_distortion', 'tsne_distortion_grupo_100', 'padrao_ouro_hbgf', 'padrao_ouro_mcla', 'word2vec-70', 'word2vec-80', 'bert-cosseno-70', 'bert-cosseno-80', 'distancia-cosseno-80', 'distancia-cosseno-70']\n",
      "\n",
      "Insira o avaliador_1:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " padrao_ouro_hbgf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insira o avaliador_2:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " distancia-cosseno-70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Carregando Av1\n",
      " --> Carregando Av2\n",
      "   --> Tamanho Av1: 4146\n",
      "   --> Tamanho Av2: 4146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2aff8ca8a3445c94f8bda68a2427fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=314.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando Persistencia\n",
      "Sucesso na inserção\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connector = MySQL_Connector(\"conn_orfeu\")\n",
    "db_name = \"flairs2\"\n",
    "conn = connector.return_conn(db_name)\n",
    "\n",
    "\n",
    "# avaliador_1 = \"padrao_ouro_mcla\"\n",
    "# avaliador_2 = \"padrao_ouro_hbgf\"\n",
    "\n",
    "avaliador_1, avaliador_2 = mostrar_boas_vindas()\n",
    "\n",
    "print(\" --> Carregando Av1\")\n",
    "df_avaliador_1 = get_df_tuplas(avaliador_1)\n",
    "print(\" --> Carregando Av2\")\n",
    "df_avaliador_2 = get_df_tuplas(avaliador_2)\n",
    "\n",
    "print(\"   --> Tamanho Av1: {}\".format(df_avaliador_1.shape[0]))\n",
    "print(\"   --> Tamanho Av2: {}\".format(df_avaliador_2.shape[0]))\n",
    "\n",
    "if df_avaliador_1.shape[0] != df_avaliador_2.shape[0]:\n",
    "    continuar = alerta_dataframe_diferente()\n",
    "    if not continuar:\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "        \n",
    "# df_avaliador_1 = df_avaliador_1.loc[df_avaliador_1[\"sentenca_id\"]== 201]\n",
    "# df_avaliador_2 = df_avaliador_2.loc[df_avaliador_2[\"sentenca_id\"]== 201]\n",
    "        \n",
    "df_score = calcular_scores(df_avaliador_1, df_avaliador_2, avaliador_1, avaliador_2, verbose=False)\n",
    "\n",
    "replace_df(df_score, \"avaliacao_similaridade_grupos\",conn)\n",
    "\n",
    "lista_sentencas = list(df_avaliador_1['sentenca_id'].unique())\n",
    "\n",
    "for sentenca_id in lista_sentencas:\n",
    "    score = calcular_media(avaliador_1, avaliador_2, sentenca_id)\n",
    "    \n",
    "    dict_df = {\n",
    "        \"sentenca_id\": [sentenca_id],\n",
    "        \"avaliador1\": [avaliador_1],\n",
    "        \"avaliador2\": [avaliador_2],\n",
    "        \"score\": score\n",
    "    }\n",
    "    df_media = pd.DataFrame(dict_df)\n",
    "    replace_df(df_media, \"avaliacao_similaridade_medias\",conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistindo Coeficientes de Concordância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>padrao_ouro</th>\n",
       "      <th>algoritmo</th>\n",
       "      <th>coerencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>All</td>\n",
       "      <td>ICC</td>\n",
       "      <td>0.8730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>All</td>\n",
       "      <td>Krippendorff</td>\n",
       "      <td>0.7513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Nmf</td>\n",
       "      <td>ICC</td>\n",
       "      <td>0.8770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Nmf</td>\n",
       "      <td>Krippendorff</td>\n",
       "      <td>0.7600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Cspa</td>\n",
       "      <td>ICC</td>\n",
       "      <td>0.8960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  padrao_ouro     algoritmo  coerencia\n",
       "0         All           ICC     0.8730\n",
       "1         All  Krippendorff     0.7513\n",
       "2         Nmf           ICC     0.8770\n",
       "3         Nmf  Krippendorff     0.7600\n",
       "4        Cspa           ICC     0.8960"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connector = MySQL_Connector(\"conn_orfeu\")\n",
    "db_name = \"flairs2\"\n",
    "conn = connector.return_conn(db_name)\n",
    "\n",
    "dict_data = {\n",
    "    \"padrao_ouro\": [\"All\", \"All\", \"Nmf\", \"Nmf\", \"Cspa\", \"Cspa\", \"Hgpa\", \"Hgpa\", \"Mcla\", \"Mcla\", \"Hgbf\", \"Hgbf\", \"Entre Avaliadores\", \"Entre Avaliadores\"],\n",
    "    \"algoritmo\": [\"ICC\", \"Krippendorff\", \"ICC\", \"Krippendorff\", \"ICC\", \"Krippendorff\", \"ICC\", \"Krippendorff\", \"ICC\", \"Krippendorff\",\"ICC\", \"Krippendorff\", \"ICC\", \"Krippendorff\"],\n",
    "    \"coerencia\": [0.873, 0.7513, 0.877, 0.760, 0.896, 0.7863, 0.893, 0.7911, 0.873, 0.7516, 0.898, 0.7917, 0.87, 0.75]\n",
    "}\n",
    "\n",
    "df_stats = pd.DataFrame(dict_data)\n",
    "\n",
    "replace_df(df_stats, \"testes_estatisticos\",conn)\n",
    "\n",
    "df_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montando dataset de testes estatísticos:\n",
    "#### - KRUSKAL-WALLIS \n",
    "#### - NEMENYI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe05d14e5d2434689762146e898d8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "connector = MySQL_Connector(\"conn_orfeu\")\n",
    "db_name = \"flairs2\"\n",
    "conn = connector.return_conn(db_name)\n",
    "\n",
    "query = '''\n",
    "\n",
    "SELECT avaliador1, avaliador2, total_grupo_av1, total_grupo_av2, grupos_iguais, grupos_iguais/total_grupo_av1 as perc\n",
    "    FROM\n",
    "    (\n",
    "        SELECT avaliador1, avaliador2, SUM(total_grupo_av1) total_grupo_av1, SUM(total_grupo_av2) total_grupo_av2\n",
    "            FROM\n",
    "            (\n",
    "                SELECT \n",
    "                        avaliador1, \n",
    "                        avaliador2, \n",
    "                        sentenca_id, \n",
    "                        MAX(grupo_av1) total_grupo_av1,\n",
    "                        MAX(grupo_av2) total_grupo_av2 \n",
    "                        FROM \n",
    "                            avaliacao_similaridade_grupos\n",
    "                        GROUP BY \n",
    "                            1,2,3\n",
    "            ) grupo_sent\n",
    "            GROUP BY 1,2\n",
    "    ) total_grupos\n",
    "    INNER JOIN\n",
    "    (\n",
    "        SELECT  \n",
    "                avaliador1,\n",
    "                avaliador2, \n",
    "                SUM(case when score >= {} then 1 else 0 end) grupos_iguais \n",
    "                FROM \n",
    "                    avaliacao_similaridade_grupos\n",
    "                GROUP BY\n",
    "                    1,2\n",
    "    ) match_grupo\n",
    "    USING (avaliador1, avaliador2)\n",
    "WHERE avaliador1 = \"padrao_ouro_mcla\"\n",
    "GROUP BY avaliador1, avaliador2\n",
    "ORDER BY grupos_iguais desc\n",
    "'''\n",
    "\n",
    "df_perc  = pd.DataFrame(columns = ['avaliador1', 'avaliador2', 'total_grupo_av1', 'total_grupo_av2', 'grupos_iguais', 'perc', 'limiar'])\n",
    "\n",
    "for i in tqdm(range(5,105,5)):\n",
    "    limiar = i/100\n",
    "    \n",
    "    df_apoio = read(conn, query.format(limiar))\n",
    "    df_apoio['limiar'] = limiar\n",
    "    df_perc = df_perc.append(df_apoio, ignore_index=True)\n",
    "    \n",
    "#df_perc = df_perc[['avaliador2', 'perc', 'limiar']].transpose()\n",
    "df_perc.to_csv(r'base_teste_estatistico_sem_transpor.csv', sep=\";\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
