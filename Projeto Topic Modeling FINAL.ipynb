{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geração de modelos de LDA \n",
    "\n",
    "## Corpus: Youtube Influencer_br\n",
    "## Versão: 3.0\n",
    "\n",
    "\n",
    "# Método: \n",
    "\n",
    "### Para encontrar o melhor k as coerências geradas serão confrontadas com as entropias dos tópicos\n",
    "\n",
    "### Serão admitidos os seguintes pos_tags: NOUN, PNOUN, ADJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from connections.mongodb_connector import Mongo_Connector\n",
    "from connections.neo4j_connector import Neo4j_Connector\n",
    "import os\n",
    "from datetime import datetime\n",
    "from gensim import corpora, models, similarities\n",
    "from models.graph_generator import Graph_Generator\n",
    "from models.tuple_extractor import Tuple_Extractor\n",
    "import pickle\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora, models, similarities\n",
    "from connections.mysql_connector import MySQL_Connector\n",
    "from acessos import read, get_conn, persistir_uma_linha, persistir_multiplas_linhas, replace_df\n",
    "from models.topic_modeling import Topic_Modeling\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "import pandas as pd\n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregando Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      video_id                                              texto\n",
       "0  --3FrTHN9qw   oi oi gente vídeo de hoje tem pede que o Emoj...\n",
       "1  --CryguVEik   [Aplausos] [Música] [Aplausos] háháhá 8 a gen...\n",
       "2  --eP6oj_gkA   contar pra vocês os três principais segredos ...\n",
       "3  --JOEggYBac   não imaginava que um dia ficar em primeiro lu...\n",
       "4  --K2l7qd7W0   Vamos falar sobre quais são os alimentos ou s..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>texto</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>--3FrTHN9qw</td>\n      <td>oi oi gente vídeo de hoje tem pede que o Emoj...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>--CryguVEik</td>\n      <td>[Aplausos] [Música] [Aplausos] háháhá 8 a gen...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>--eP6oj_gkA</td>\n      <td>contar pra vocês os três principais segredos ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>--JOEggYBac</td>\n      <td>não imaginava que um dia ficar em primeiro lu...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>--K2l7qd7W0</td>\n      <td>Vamos falar sobre quais são os alimentos ou s...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "connector = MySQL_Connector(\"conn_orfeu\")\n",
    "\n",
    "\n",
    "def buscar_videos_transcripions(conn, idioma, transcription_type=\"\", limit=\"\"):\n",
    "    transcription_language = \"transcription_pt\" if idioma ==\"pt-br\" else \"transcription_en\"\n",
    "    query = \"SELECT  video_id, {} as texto FROM video_transcriptions WHERE transcription_type <> 'Erro'\".format(transcription_language)\n",
    "\n",
    "    if transcription_type != \"\":\n",
    "        sub_query = \" AND transcription_type = '{}'\".format(transcription_type)\n",
    "        query += sub_query\n",
    "    \n",
    "    if limit != \"\":\n",
    "        sub_query = \" LIMIT {}\".format(limit)\n",
    "        query += sub_query\n",
    "\n",
    "    df_videos_trans = read(conn, query)\n",
    "    df_videos_trans.columns=['video_id', \"texto\"]\n",
    "\n",
    "    return df_videos_trans\n",
    "\n",
    "conn = connector.return_conn(\"influencer_br\")\n",
    "\n",
    "df_videos_trans = buscar_videos_transcripions(conn, \"pt-br\", limit=10)\n",
    "df_videos_trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando nome do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projetos\\Mestrado\\Projeto PDM\\v4\\YouGraph\\sistema\\Python\\modelos_lda\n",
      "09-04-2021 19_12_37\n"
     ]
    }
   ],
   "source": [
    "date_string = datetime.now().strftime(\"%d-%m-%Y %H_%M_%S\")\n",
    "path = os.getcwd()\n",
    "path_modelo = \"{}\\modelos_lda\".format(path)\n",
    "print(path_modelo)\n",
    "print(date_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametros do Pré Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "    stop_words_list = [\"aqui\", \"gente\", \"oi\", \"então\", \"ai\", \"aí\", \"ufa\", \"coisa\", \"[Aplausos]\", \"[Música]\", \"né\",\"tá\", \"vídeo\",\n",
    "                       \"casar\", \"coisa\",\"canal\", \"tipo\", \"vezes\", \"mano\", \"meio\", \"ea\", \"eo\", \"só\", \"ae\", \"eis\", \"caraca\", \"caramba\"\n",
    "                      ]\n",
    "allowed_postags = [\"NOUN\",\"PROPN\", \"VERB\", \"ADJ\"]\n",
    "min_count=10\n",
    "threshold=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciando Obj Topic_Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeling = Topic_Modeling(language=\"pt-br\",stop_words_list=stop_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando lista dos documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_documentos = df_videos_trans['texto'].tolist()\n",
    "# lista_documentos[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrando pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_documentos = [topic_modeling.filtrar_pos_tag(texto) for texto in lista_documentos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré Processamentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_documentos_tokenizado = topic_modeling.tokenizar(lista_documentos)\n",
    "lista_documentos_tokenizado[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contanto o número de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Num de tokens: 128,868,340'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def contar_tokens(lista_documentos):\n",
    "    num_tokens = 0\n",
    "    #lista_documentos.map(lambda doc: print(type(doc)) )\n",
    "    for doc in lista_documentos:\n",
    "        num_tokens = num_tokens + len(doc)\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_sem_processar = contar_tokens(lista_documentos)\n",
    " \n",
    "'Num de tokens: {:,}'.format(num_tokens_sem_processar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_documentos_tokenizado_stop_words = topic_modeling.remover_stop_words(lista_documentos_tokenizado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando BiGramas e TriGramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_documento_bi_gram, lista_documento_tri_gram = topic_modeling.montar_n_grams(lista_documentos_tokenizado_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrando os Bi e Tri Gramas (DE UM JEITO MTO GAMBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_gramas(lista_documentos_grams):\n",
    "    lista_grams = []\n",
    "    for doc in lista_documentos_grams:\n",
    "        for token in doc:\n",
    "            if token.find(\"_\") != -1:\n",
    "                lista_grams.append(token)\n",
    "    return lista_grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_tri_gramas = encontrar_gramas(lista_documento_tri_gram)\n",
    "lista_bi_gramas = encontrar_gramas(lista_documento_bi_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aplausos',\n",
       " 'música',\n",
       " 'aplausos',\n",
       " 'háháhá',\n",
       " 'manias',\n",
       " 'amor',\n",
       " 'maior',\n",
       " 'verdade',\n",
       " 'muro',\n",
       " 'anos']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lista_bi_gramas[:50]\n",
    "lista_tri_gramas[:50]\n",
    "lista_documento_tri_gram[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematizando os tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_documento_lematizada = topic_modeling.lematizar_documentos(lista_documento_tri_gram) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numero de tokens após processamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num de tokens: 128,868,340\n",
      "Num de tokens após Stop Words: 16,493,979\n",
      "Num de tokens após com bi_grams: 15,868,240\n",
      "Num de tokens após com tri_grams: 15,746,434\n",
      "Num de tokens após lematização: 15,210,559\n"
     ]
    }
   ],
   "source": [
    "num_tokens_sem_processar = contar_tokens(lista_documentos)\n",
    "print('Num de tokens: {:,}'.format(num_tokens_sem_processar))\n",
    "\n",
    "num_tokens_stop_words = contar_tokens(lista_documentos_tokenizado_stop_words)\n",
    "print('Num de tokens após Stop Words: {:,}'.format(num_tokens_stop_words))\n",
    "\n",
    "\n",
    "num_tokens_bi_gram  = contar_tokens(lista_documento_bi_gram)\n",
    "num_tokens_tri_gram = contar_tokens(lista_documento_tri_gram)\n",
    "print('Num de tokens após com bi_grams: {:,}'.format(num_tokens_bi_gram))\n",
    "print('Num de tokens após com tri_grams: {:,}'.format(num_tokens_tri_gram))\n",
    "\n",
    "num_tokens_lematizado = contar_tokens(lista_documento_lematizada)\n",
    "print(\"Num de tokens após lematização: {:,}\".format(num_tokens_lematizado))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando o Id2Word    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = topic_modeling.montar_id2word(lista_documento_lematizada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando o Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(153687 unique tokens: ['alice', 'alto', 'alçar', 'amarelar', 'amarelo']...)\n"
     ]
    }
   ],
   "source": [
    "corpus = topic_modeling.montar_novo_corpus(lista_documento_lematizada, id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando o ID2WORD e Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_corpus = {\n",
    "    \"lista_documentos_tokenizado\": lista_documentos_tokenizado,\n",
    "    \"lista_documentos_tokenizado_stop_words\": lista_documentos_tokenizado_stop_words,\n",
    "    \"lista_documentos_lematizada\":lista_documento_lematizada,\n",
    "    \"lista_documento_tri_gram\": lista_documento_tri_gram,\n",
    "    \"lista_tri_gramas\": lista_tri_gramas,\n",
    "    \"id2word\":id2word,\n",
    "    \"corpus\":corpus\n",
    "    \n",
    "}\n",
    "\n",
    "save_path = \"{}\\\\{}\".format(path_modelo, date_string)\n",
    "os.mkdir(save_path) \n",
    "with open(\"{}\\\\dict_corpus.pickle\".format(save_path), 'wb') as handle: #SALVANDO DICT EM UM PICKLE\n",
    "    pickle.dump(dict_corpus, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento dos Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Lendo Pickle\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"09-11-2020 18_33_23\"\n",
    "\n",
    "path = os.getcwd()\n",
    "path_modelo = \"{}\\modelos_lda\\{}\".format(path, folder_name)\n",
    "\n",
    "print(\"-> Lendo Pickle\")\n",
    "with (open(\"{}\\\\dict_corpus.pickle\".format(path_modelo), \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            dict_corpus = pickle.load(openfile)\n",
    "        except EOFError:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dict_corpus['corpus']\n",
    "id2word= dict_corpus['id2word']\n",
    "lista_documentos_tokenizado_stop_words = dict_corpus['lista_documentos_tokenizado_stop_words']\n",
    "lista_documento_lematizada = dict_corpus['lista_documentos_lematizada']\n",
    "lista_tri_gramas = dict_corpus[\"lista_tri_gramas\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carregando MALLET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Projetos\\\\Mestrado\\\\mallet-2.0.8\\\\bin\\\\mallet'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mallet_path = \"D:\\Projetos\\Mestrado\\mallet-2.0.8\"\n",
    "os.environ['MALLET_HOME'] = mallet_path\n",
    "\n",
    "mallet_path = \"{}\\\\bin\\mallet\".format(mallet_path)\n",
    "\n",
    "mallet_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 10\n",
    "limit = 120\n",
    "step= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando novo modelo...\n",
      "Novo modelo Gerado..\n",
      "Calculando coerencia\n",
      " --> Pronto\n",
      "num topics: 10\n",
      "*********\n",
      "[0.3921452783749029]\n",
      "Gerando novo modelo...\n",
      "Novo modelo Gerado..\n",
      "Calculando coerencia\n",
      " --> Pronto\n",
      "num topics: 20\n",
      "*********\n",
      "[0.3921452783749029, 0.43399610148056356]\n",
      "Gerando novo modelo...\n",
      "Novo modelo Gerado..\n",
      "Calculando coerencia\n",
      " --> Pronto\n",
      "num topics: 30\n",
      "*********\n",
      "[0.3921452783749029, 0.43399610148056356, 0.4418800677066677]\n",
      "Gerando novo modelo...\n",
      "Novo modelo Gerado..\n",
      "Calculando coerencia\n",
      " --> Pronto\n",
      "num topics: 40\n",
      "*********\n",
      "[0.3921452783749029, 0.43399610148056356, 0.4418800677066677, 0.4536300600784065]\n",
      "Gerando novo modelo...\n",
      "Novo modelo Gerado..\n",
      "Calculando coerencia\n",
      " --> Pronto\n",
      "num topics: 50\n",
      "*********\n",
      "[0.3921452783749029, 0.43399610148056356, 0.4418800677066677, 0.4536300600784065, 0.4477756605622723]\n",
      "Gerando novo modelo...\n",
      "Novo modelo Gerado..\n",
      "Calculando coerencia\n",
      " --> Pronto\n",
      "num topics: 60\n",
      "*********\n",
      "[0.3921452783749029, 0.43399610148056356, 0.4418800677066677, 0.4536300600784065, 0.4477756605622723, 0.46121620611437625]\n",
      "Gerando novo modelo...\n",
      "Novo modelo Gerado..\n",
      "Calculando coerencia\n",
      " --> Pronto\n",
      "num topics: 70\n",
      "*********\n",
      "[0.3921452783749029, 0.43399610148056356, 0.4418800677066677, 0.4536300600784065, 0.4477756605622723, 0.46121620611437625, 0.4593699166129623]\n",
      "Gerando novo modelo...\n",
      "Novo modelo Gerado..\n",
      "Calculando coerencia\n",
      " --> Pronto\n",
      "num topics: 80\n",
      "*********\n",
      "[0.3921452783749029, 0.43399610148056356, 0.4418800677066677, 0.4536300600784065, 0.4477756605622723, 0.46121620611437625, 0.4593699166129623, 0.4641071011991242]\n",
      "Gerando novo modelo...\n",
      "Novo modelo Gerado..\n",
      "Calculando coerencia\n",
      " --> Pronto\n",
      "num topics: 90\n",
      "*********\n",
      "[0.3921452783749029, 0.43399610148056356, 0.4418800677066677, 0.4536300600784065, 0.4477756605622723, 0.46121620611437625, 0.4593699166129623, 0.4641071011991242, 0.4622511691698248]\n",
      "Gerando novo modelo...\n",
      "Novo modelo Gerado..\n",
      "Calculando coerencia\n",
      " --> Pronto\n",
      "num topics: 100\n",
      "*********\n",
      "[0.3921452783749029, 0.43399610148056356, 0.4418800677066677, 0.4536300600784065, 0.4477756605622723, 0.46121620611437625, 0.4593699166129623, 0.4641071011991242, 0.4622511691698248, 0.4561780344894651]\n",
      "Gerando novo modelo...\n",
      "Novo modelo Gerado..\n",
      "Calculando coerencia\n",
      " --> Pronto\n",
      "num topics: 110\n",
      "*********\n",
      "[0.3921452783749029, 0.43399610148056356, 0.4418800677066677, 0.4536300600784065, 0.4477756605622723, 0.46121620611437625, 0.4593699166129623, 0.4641071011991242, 0.4622511691698248, 0.4561780344894651, 0.4608962384365008]\n"
     ]
    }
   ],
   "source": [
    "coherence_values = []\n",
    "model_list = []\n",
    "for num_topics in range(start, limit, step):\n",
    "    print(\"Gerando novo modelo...\")\n",
    "\n",
    "#     model = LdaMulticore(corpus=corpus, \n",
    "#                         id2word=id2word,\n",
    "#                         random_state=100, \n",
    "#                         num_topics=num_topics,\n",
    "#                         per_word_topics=True,\n",
    "#                         workers=3)\n",
    "    \n",
    "    model = LdaMallet(mallet_path,\n",
    "                      corpus=corpus, \n",
    "                      num_topics=num_topics,\n",
    "                      workers=2,\n",
    "                      id2word=id2word)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Novo modelo Gerado..\")\n",
    "    print(\"Calculando coerencia\")\n",
    "    \n",
    "    coherencemodel = CoherenceModel(model=model, texts=lista_documento_lematizada, dictionary=id2word, coherence='c_v')\n",
    "    #coherencemodel = CoherenceModel(model=model, texts=lista_documento_lematizada, dictionary=id2word, coherence='c_npmi')\n",
    "    \n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "    print(\" --> Pronto\")\n",
    "\n",
    "    model.save(\"{}\\#_{}\".format(path_modelo,num_topics))\n",
    "    \n",
    "    model_list.append(model)\n",
    "    \n",
    "    #coherence_values = coherence_values\n",
    "    print(\"num topics: {}\".format(num_topics))\n",
    "    print(\"*********\")\n",
    "    print(coherence_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3921452783749029,\n",
       " 0.43399610148056356,\n",
       " 0.4418800677066677,\n",
       " 0.4536300600784065,\n",
       " 0.4477756605622723,\n",
       " 0.46121620611437625,\n",
       " 0.4593699166129623,\n",
       " 0.4641071011991242,\n",
       " 0.4622511691698248,\n",
       " 0.4561780344894651,\n",
       " 0.4608962384365008]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retornando as top palavras dos tópicos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_analise = model_list[0]\n",
    "\n",
    "df_palavras, dict_palavras_topicos = topic_modeling.retornar_top_key_words(modelo_analise, num_palavras=30)\n",
    "\n",
    "#df_palavras.head(50)\n",
    "df_palavras.to_csv(\"09-11-2020 18_33_23 - 50 tópicos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'09-11-2020 18_33_23'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_word</th>\n",
       "      <th>count</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>266</td>\n",
       "      <td>180076</td>\n",
       "      <td>contar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>357</td>\n",
       "      <td>132506</td>\n",
       "      <td>doença</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>52</td>\n",
       "      <td>123100</td>\n",
       "      <td>perfazer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>275</td>\n",
       "      <td>121414</td>\n",
       "      <td>matemático</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>26</td>\n",
       "      <td>108945</td>\n",
       "      <td>grande</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>121</td>\n",
       "      <td>106127</td>\n",
       "      <td>álcool_gel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>185</td>\n",
       "      <td>101435</td>\n",
       "      <td>aparência</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>391</td>\n",
       "      <td>79613</td>\n",
       "      <td>puro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>118</td>\n",
       "      <td>77616</td>\n",
       "      <td>zinho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>363</td>\n",
       "      <td>75844</td>\n",
       "      <td>mês</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>80</td>\n",
       "      <td>74930</td>\n",
       "      <td>imaginação</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>801</td>\n",
       "      <td>72783</td>\n",
       "      <td>borracho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>36</td>\n",
       "      <td>72192</td>\n",
       "      <td>parceria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>129</td>\n",
       "      <td>71878</td>\n",
       "      <td>camada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>95</td>\n",
       "      <td>65464</td>\n",
       "      <td>pronto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>78</td>\n",
       "      <td>63479</td>\n",
       "      <td>tampar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>260</td>\n",
       "      <td>62574</td>\n",
       "      <td>axé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>170</td>\n",
       "      <td>56625</td>\n",
       "      <td>beijar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>267</td>\n",
       "      <td>51525</td>\n",
       "      <td>capacitação</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>259</td>\n",
       "      <td>48983</td>\n",
       "      <td>amêndoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>753</td>\n",
       "      <td>48253</td>\n",
       "      <td>agudos_cristalinos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>334</td>\n",
       "      <td>47992</td>\n",
       "      <td>deus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>92</td>\n",
       "      <td>44521</td>\n",
       "      <td>ideal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>476</td>\n",
       "      <td>44432</td>\n",
       "      <td>pensamento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>109</td>\n",
       "      <td>43420</td>\n",
       "      <td>caixa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>71</td>\n",
       "      <td>43354</td>\n",
       "      <td>triângulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1085</td>\n",
       "      <td>1159</td>\n",
       "      <td>42843</td>\n",
       "      <td>escadaria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>169</td>\n",
       "      <td>42171</td>\n",
       "      <td>colar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>85</td>\n",
       "      <td>41090</td>\n",
       "      <td>interno</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>104</td>\n",
       "      <td>40714</td>\n",
       "      <td>dica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>54</td>\n",
       "      <td>40203</td>\n",
       "      <td>passar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>247</td>\n",
       "      <td>39945</td>\n",
       "      <td>desenrolar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>365</td>\n",
       "      <td>39753</td>\n",
       "      <td>número</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>344</td>\n",
       "      <td>39631</td>\n",
       "      <td>realização</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>328</td>\n",
       "      <td>39055</td>\n",
       "      <td>pessoa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>290</td>\n",
       "      <td>38304</td>\n",
       "      <td>certeza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1594</td>\n",
       "      <td>1385</td>\n",
       "      <td>37396</td>\n",
       "      <td>áfrico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>143</td>\n",
       "      <td>37162</td>\n",
       "      <td>joinha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>604</td>\n",
       "      <td>35323</td>\n",
       "      <td>companhia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>523</td>\n",
       "      <td>35286</td>\n",
       "      <td>corpo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>845</td>\n",
       "      <td>35133</td>\n",
       "      <td>corte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>643</td>\n",
       "      <td>714</td>\n",
       "      <td>34313</td>\n",
       "      <td>assinatura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>261</td>\n",
       "      <td>33079</td>\n",
       "      <td>coreografia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>18</td>\n",
       "      <td>33009</td>\n",
       "      <td>cnc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>358</td>\n",
       "      <td>32579</td>\n",
       "      <td>fulano_fulano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>601</td>\n",
       "      <td>32117</td>\n",
       "      <td>resgatar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>112</td>\n",
       "      <td>31793</td>\n",
       "      <td>coisa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>135</td>\n",
       "      <td>31644</td>\n",
       "      <td>sugestão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>841</td>\n",
       "      <td>737</td>\n",
       "      <td>31307</td>\n",
       "      <td>qualidade_sonora</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>348</td>\n",
       "      <td>30260</td>\n",
       "      <td>cultura</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_word   count                word\n",
       "280       266  180076              contar\n",
       "295       357  132506              doença\n",
       "133        52  123100            perfazer\n",
       "333       275  121414          matemático\n",
       "78         26  108945              grande\n",
       "179       121  106127          álcool_gel\n",
       "186       185  101435           aparência\n",
       "367       391   79613                puro\n",
       "177       118   77616               zinho\n",
       "345       363   75844                 mês\n",
       "84         80   74930          imaginação\n",
       "660       801   72783            borracho\n",
       "128        36   72192            parceria\n",
       "22        129   71878              camada\n",
       "140        95   65464              pronto\n",
       "156        78   63479              tampar\n",
       "188       260   62574                 axé\n",
       "14        170   56625              beijar\n",
       "271       267   51525         capacitação\n",
       "184       259   48983             amêndoa\n",
       "626       753   48253  agudos_cristalinos\n",
       "290       334   47992                deus\n",
       "81         92   44521               ideal\n",
       "535       476   44432          pensamento\n",
       "21        109   43420               caixa\n",
       "163        71   43354           triângulo\n",
       "1085     1159   42843           escadaria\n",
       "39        169   42171               colar\n",
       "87         85   41090             interno\n",
       "53        104   40714                dica\n",
       "130        54   40203              passar\n",
       "209       247   39945          desenrolar\n",
       "349       365   39753              número\n",
       "369       344   39631          realização\n",
       "357       328   39055              pessoa\n",
       "277       290   38304             certeza\n",
       "1594     1385   37396              áfrico\n",
       "90        143   37162              joinha\n",
       "598       604   35323           companhia\n",
       "455       523   35286               corpo\n",
       "688       845   35133               corte\n",
       "643       714   34313          assinatura\n",
       "201       261   33079         coreografia\n",
       "35         18   33009                 cnc\n",
       "315       358   32579       fulano_fulano\n",
       "614       601   32117            resgatar\n",
       "36        112   31793               coisa\n",
       "152       135   31644            sugestão\n",
       "841       737   31307    qualidade_sonora\n",
       "285       348   30260             cultura"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorted_d = dict( sorted(id2word.cfs.items(), key=operator.itemgetter(1),reverse=True))\n",
    "# sorted_d\n",
    "\n",
    "data_contagem = {'id_word': list(id2word.cfs.keys()), 'count': list(id2word.cfs.values())}\n",
    "df_palavras_count = pd.DataFrame.from_dict(data_contagem)\n",
    "\n",
    "\n",
    "data_id2token = {'id_word': list(id2word.cfs.keys()), 'word': list(id2word.id2token.values())}\n",
    "df_id2token = pd.DataFrame.from_dict(data_id2token)\n",
    "\n",
    "\n",
    "\n",
    "df_palavras_count = pd.merge(df_palavras_count, df_id2token, on='id_word')\n",
    "\n",
    "df_count_palavras.sort_values(by=['count'], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entendendo os significados de palavras bizaras (VIDAR, VEZAR, PELAR, MANIR, GATAR, MEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_alvo = [\"casar\"]\n",
    "\n",
    "for sent in lista_documentos_tokenizado_stop_words:\n",
    "            doc = topic_modeling.nlp(\" \".join(sent))   \n",
    "            for token in doc :\n",
    "                lemma = (token.lemma_)\n",
    "                if lemma in lista_alvo:\n",
    "                    print(\"Token {} - Lemma {}\".format(token, lemma))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_documento_lematizada = topic_modeling.remover_stop_words(lista_documento_lematizada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relatório dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_tokens_lematizado' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-cd63569b4742>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnum_tokens_lematizado\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'num_tokens_lematizado' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd04297d422904d548e0e5f6b7f99937aeafed788f79e45c0e579ca72c9146b21eb",
   "display_name": "Python 3.8.5 64-bit ('gensim': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}